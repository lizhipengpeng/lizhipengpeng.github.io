<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="主要学习如何对大模型进行预训练工作，如果我们要使用一个已经经过一部分预训练的模型的话，实际上我们就可以得到这个预训练模型的预训练代码 1. 什么是大模型参数量很大（十几亿，几百亿）的深度神经网络模型 2. 预训练是什么意思预训练这个词并不是一个很新的词，之前就有，我理解的预训练就是大模型在某一个任务上进行模型训练，训练完成的模型参数就是预训练模型，预训练模型就意味着把人类的语言知识，先学了一个东西">
<meta property="og:type" content="article">
<meta property="og:title" content="大语言模型的预训练技术">
<meta property="og:url" content="http://example.com/2023/05/30/pre-trained%20model/index.html">
<meta property="og:site_name" content="随笔">
<meta property="og:description" content="主要学习如何对大模型进行预训练工作，如果我们要使用一个已经经过一部分预训练的模型的话，实际上我们就可以得到这个预训练模型的预训练代码 1. 什么是大模型参数量很大（十几亿，几百亿）的深度神经网络模型 2. 预训练是什么意思预训练这个词并不是一个很新的词，之前就有，我理解的预训练就是大模型在某一个任务上进行模型训练，训练完成的模型参数就是预训练模型，预训练模型就意味着把人类的语言知识，先学了一个东西">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/home/killer/blog_tircle/assets/2023-06-07_19-48.png">
<meta property="article:published_time" content="2023-05-30T08:02:39.348Z">
<meta property="article:modified_time" content="2023-06-07T13:53:32.212Z">
<meta property="article:author" content="李治澎">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/home/killer/blog_tircle/assets/2023-06-07_19-48.png">

<link rel="canonical" href="http://example.com/2023/05/30/pre-trained%20model/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-Hans'
  };
</script>

  <title>大语言模型的预训练技术 | 随笔</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">随笔</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/05/30/pre-trained%20model/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="李治澎">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="随笔">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          大语言模型的预训练技术
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-05-30 16:02:39" itemprop="dateCreated datePublished" datetime="2023-05-30T16:02:39+08:00">2023-05-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-06-07 21:53:32" itemprop="dateModified" datetime="2023-06-07T21:53:32+08:00">2023-06-07</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>主要学习如何对大模型进行预训练工作，如果我们要使用一个已经经过一部分预训练的模型的话，实际上我们就可以得到这个预训练模型的预训练代码</p>
<h2 id="1-什么是大模型"><a href="#1-什么是大模型" class="headerlink" title="1. 什么是大模型"></a>1. 什么是大模型</h2><p>参数量很大（十几亿，几百亿）的深度神经网络模型</p>
<h2 id="2-预训练是什么意思"><a href="#2-预训练是什么意思" class="headerlink" title="2. 预训练是什么意思"></a>2. 预训练是什么意思</h2><p>预训练这个词并不是一个很新的词，之前就有，我理解的预训练就是大模型在某一个任务上进行模型训练，训练完成的模型参数就是预训练模型，预训练模型就意味着把人类的语言知识，先学了一个东西，然后再代入到某个具体任务，就顺手了，就是这么一个简单的道理。</p>
<p>预训练思想的本质：</p>
<ol>
<li>模型参数不再是随机初始化，而是通过一些任务（如语言模型）进行预训练</li>
<li>将训练任务拆解成共性学习和特性学习两个步骤</li>
</ol>
<h2 id="3-预训练有哪些技术"><a href="#3-预训练有哪些技术" class="headerlink" title="3. 预训练有哪些技术"></a>3. 预训练有哪些技术</h2><p>那么预训练应该具体怎么做？</p>
<p>大致流程：自监督的大规模预训练 + 微调，本文重点关注自监督预训练如何实现</p>
<p>预训练本质上是迁移学习的一种应用，利用几乎无限的文本，学习输入句子的每一个成员的上下文相关的表示，它隐式地学习到了通用的语法语义知识，预训练通过自监督学习从大规模数据中获得与具体任务无关的预训练模型。体现某一个词在一个特定上下文中的语义表征。</p>
<h3 id="3-1-网络"><a href="#3-1-网络" class="headerlink" title="3.1 网络"></a>3.1 网络</h3><p>transformer模型，是预训练的核心网络，因为有个非常好的优点，就是可以跑得很快，并且做的很深。</p>
<ol>
<li>产品形态设计，闲聊交互式，有来源</li>
<li>知识图谱</li>
<li>算法  抽取关系 抽取实体 ==算法选型+理由 跑出来个demo==<ol>
<li>bert + 下游任务</li>
<li>gpt4 + prompt</li>
<li>nlp模型 专用</li>
</ol>
</li>
</ol>
<h3 id="3-2-预训练任务"><a href="#3-2-预训练任务" class="headerlink" title="3.2 预训练任务"></a>3.2 预训练任务</h3><p><img src="/home/killer/blog_tircle/assets/2023-06-07_19-48.png"></p>
<p>预训练任务的分类架构通常可以分为两类，一类是基于自监督学习的预训练任务，另一类是基于监督学习的预训练任务。具体而言，可以将预训练任务分成以下几类：</p>
<ol>
<li><p>基于自监督学习的预训练任务：这种预训练任务要求模型通过自监督方式来学习数据中的模式和结构，而不需要人工标注的标签。这种方法通常包括自回归预测和掩码语言模型等任务，模型通过对输入文本的预测来学习文本的语言结构和语义信息。</p>
<ol>
<li><p>自回归预测</p>
<ul>
<li>给出前几个单词，预测后一个单词的概率</li>
<li>从左向右做预测，再从右往左做预测</li>
</ul>
</li>
<li><p>掩码语言模型</p>
<ul>
<li><p>将句子中的一个词语掩盖掉，预测该词</p>
</li>
<li><p>动态mask（RoBERTa）</p>
</li>
<li><p>SpanBERT：Random Contiguous Words Masking and Span Boundary Objective (SBO) 随机掩盖一段连续的词、模型在预测掩盖词的同时，还需要预测出掩盖词所在的连续片段的开始位置和结束位置。</p>
</li>
<li><p>StructBERT ：Span Order Recovery task 模型在输入的文本中找到两个连续的实体，并预测它们在原始文本中的顺序</p>
</li>
<li><p>TLM：XLM 双语对齐 将源语言句子和目标语言句子拼接，模型需要预测这个拼接后的句子中缺失的一些词汇或者短语。</p>
</li>
<li><p>Seq2Seq MLM</p>
<blockquote>
<p>对于句子”The quick brown fox jumps over the lazy dog.”，Seq2Seq MLM任务可能会将其掩码为”The quick [MASK] fox jumps [MASK] the lazy dog.”，然后让模型预测掩码位置上的词汇。在预测第一个掩码位置时，模型可能会将其预测为”brown”，然后将”brown”作为下一个掩码位置的输入，继续预测下一个掩码位置上的词汇。</p>
</blockquote>
</li>
</ul>
</li>
<li><p>PLM：Permuted Language Modeling模型输入一个随机排列的词序列，并预测这些词在原始序列中的正确顺序。</p>
</li>
<li><p>对比学习Contrastive Learning，没有被替换的句子得分比被替换的句子得分高</p>
</li>
<li><p>DAE：Denoising Autoencoder：输入一句有噪音的句子，输入没有噪音的句子</p>
<blockquote>
<ol>
<li>随机mask</li>
<li>随机删除token</li>
<li>随机去除n个token</li>
<li>句子随机排序</li>
<li>文档旋转</li>
</ol>
</blockquote>
</li>
</ol>
</li>
<li><p>基于监督学习的预训练任务：这种预训练任务要求模型通过有标注的数据来学习任务特定的语言处理技能。这些任务通常包括情感分析、命名实体识别、文本分类等任务，模型通过对标注数据的学习来提高在特定任务上的表现。==NLP领域，没有足够多的带标签的数据==</p>
</li>
<li><p>基于弱监督学习的预训练任务：这种预训练任务要求模型通过仅有部分标注数据或者弱标注数据来学习任务特定的语言处理技能。这些任务通常包括半监督学习、多任务学习等方法，模型通过对标注数据和非标注数据的学习来提高在特定任务上的表现。</p>
</li>
</ol>
<p>以上是从头开始训练一个模型，如果要在一个已经预训练过的模型上再次进行训练，那就会面临<strong>灾难性遗忘</strong>的问题，以下是如何应对灾难性遗忘的方法，也被称为<strong>增量学习</strong>，与在线学习也有点关系：</p>
<ol>
<li>intelligent synapse（冻结权重）</li>
<li>replay</li>
<li>meta learning</li>
</ol>
<h3 id="3-3-微调"><a href="#3-3-微调" class="headerlink" title="3.3 微调"></a>3.3 微调</h3><h2 id="4-实践环节"><a href="#4-实践环节" class="headerlink" title="4. 实践环节"></a>4. 实践环节</h2><p>总结一下，目前的大模型范式基本上都是预训练+微调</p>
<p>预训练分为两种情况：</p>
<ol>
<li><p>从头开始预训练</p>
<p>那就是要构造训练任务，主要用自回归任务和自编码任务两种主流训练方法</p>
</li>
<li><p>对已经预训练过的模型进行再次预训练</p>
<p>这种情况可以当作增量学习的问题来看</p>
</li>
</ol>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/05/29/Latex/" rel="prev" title="《简单高效LaTeX》">
      <i class="fa fa-chevron-left"></i> 《简单高效LaTeX》
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/06/08/%E5%A6%82%E4%BD%95%E6%8C%91%E9%80%89%E8%A1%A3%E6%9C%8D/" rel="next" title="如何挑选好看的衣服">
      如何挑选好看的衣服 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E4%BB%80%E4%B9%88%E6%98%AF%E5%A4%A7%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.</span> <span class="nav-text">1. 什么是大模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%98%AF%E4%BB%80%E4%B9%88%E6%84%8F%E6%80%9D"><span class="nav-number">2.</span> <span class="nav-text">2. 预训练是什么意思</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%9C%89%E5%93%AA%E4%BA%9B%E6%8A%80%E6%9C%AF"><span class="nav-number">3.</span> <span class="nav-text">3. 预训练有哪些技术</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-%E7%BD%91%E7%BB%9C"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-%E9%A2%84%E8%AE%AD%E7%BB%83%E4%BB%BB%E5%8A%A1"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 预训练任务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-%E5%BE%AE%E8%B0%83"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 微调</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E5%AE%9E%E8%B7%B5%E7%8E%AF%E8%8A%82"><span class="nav-number">4.</span> <span class="nav-text">4. 实践环节</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">李治澎</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">李治澎</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
